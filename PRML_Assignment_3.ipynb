{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCHewbZ5Xjhh"
   },
   "source": [
    "# Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q2mFksH8XT2u",
    "outputId": "c0271876-beae-4103-8cdf-39a841fb7cb2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import math\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3u5p_bxkMB8"
   },
   "source": [
    "# Importing the Spam Mails dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8cg4oavgecP",
    "outputId": "fbc4a95c-f096-4b20-e5e1-62831012d840"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1035\n"
     ]
    }
   ],
   "source": [
    "df =   pd.read_csv('https://raw.githubusercontent.com/GaneshDev2003/PRML-Assignment-3/main/spam_ham_dataset.csv')\n",
    "df.columns = ['index', 'label', 'text', 'label_num']\n",
    "X = df['text'].values\n",
    "y = df['label_num'].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=0)\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFKI6zZjsF-A"
   },
   "source": [
    "# Preprocessing\n",
    "The email text is preprocessed using the following steps:\n",
    "\n",
    "\n",
    "1.  Punctuation removal\n",
    "2.  Stopword removal\n",
    "3.  Feature vector extraction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JQqEZ1E7siuM"
   },
   "outputs": [],
   "source": [
    "def remove_punctutations(word):\n",
    "  list_of_punctutations = [\",\", \".\", \"?\", \"!\", \"'\", \"+\", \"(\", \")\", \"{\", \"}\", \";\" , \"\\\\\", \"/\", \":\", \"-\", \"=\", \"_\", \"@\", \"$\", \"%\", \"^\", \"&\", \"*\", \"#\"]\n",
    "  new_word = \"\"\n",
    "  for character in word:\n",
    "    if(character not in list_of_punctutations):\n",
    "      new_word = new_word + character\n",
    "  return new_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "i9tVIQwet0nQ"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(list_of_words):\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  filtered_words = []\n",
    "  for word in list_of_words:\n",
    "    if(word.lower() not in stop_words):\n",
    "      filtered_words.append(word)\n",
    "  return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9l8KY71-2GcW"
   },
   "outputs": [],
   "source": [
    "def remove_digits(word):\n",
    "  return(''.join([i for i in word if not i.isdigit()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2v8dpz6eARsY"
   },
   "outputs": [],
   "source": [
    "def preprocess(email):\n",
    "  tokens = []\n",
    "  for word in email.split():\n",
    "    word = remove_punctutations(word)\n",
    "    word = remove_digits(word)\n",
    "    tokens.append(word.lower())\n",
    "  tokens = remove_stopwords(tokens)\n",
    "  tokens = [token for token in tokens if token]\n",
    "  tokens = set(tokens)\n",
    "  \n",
    "  return list(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_rCKiyyYsDGJ",
    "outputId": "2978f2bb-d63f-404d-cbd9-710de911732a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39133\n"
     ]
    }
   ],
   "source": [
    "dictionary = []\n",
    "tokens_list = []\n",
    "for email in X_train:\n",
    "  tokens = preprocess(email)\n",
    "  tokens_list.append(tokens)\n",
    "  dictionary = dictionary + tokens\n",
    "dictionary = list(set(dictionary))\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hWz2AScOllcP"
   },
   "outputs": [],
   "source": [
    "def extract_feature_vector(email_tokens, unique_words):\n",
    "  temp = np.arange(len(unique_words))\n",
    "  j=0\n",
    "  for word in unique_words:\n",
    "    temp[j] = int(word in email_tokens)\n",
    "    j = j+1\n",
    "  return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mv3Q4jPRfn2c"
   },
   "outputs": [],
   "source": [
    "feature_vec = np.zeros((len(tokens_list), len(dictionary)))\n",
    "i=0\n",
    "for email_tokens in tokens_list:\n",
    "  feature_vec[i] = extract_feature_vector(email_tokens, dictionary)\n",
    "  i = i+1\n",
    "feature_vec = np.array(feature_vec, dtype = object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ9uZEu_sAFl"
   },
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j81D69jjmy4M",
    "outputId": "ec2cfd7b-b73f-47c7-8b81-27cc1a2f4903",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_spam_indices = [i for i in range(len(X_train)) if y_train[i] == 1]\n",
    "train_ham_indices = [i for i in range(len(X_train)) if y_train[i] == 0]\n",
    "train_spam_tokens = [tokens_list[i] for i in train_spam_indices]\n",
    "train_ham_tokens = [tokens_list[i] for i in train_ham_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rFNoaYgS7G3l",
    "outputId": "6949240e-4424-416e-f808-33588a7043fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1196\n",
      "2940\n",
      "0.28916827852998067\n"
     ]
    }
   ],
   "source": [
    "prob_spam_ham = len(train_spam_indices)/(len(train_spam_indices)+ len(train_ham_indices))\n",
    "print(len(train_spam_indices))\n",
    "print(len(train_ham_indices))\n",
    "print(prob_spam_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ZmvS1N8U-nZg"
   },
   "outputs": [],
   "source": [
    "def predict(email):\n",
    "  test_email_tokens = preprocess(email)\n",
    "  n = 0\n",
    "  prob_spam = math.log(prob_spam_ham)\n",
    "  prob_ham = math.log(1-prob_spam_ham)\n",
    "  for token in test_email_tokens:\n",
    "    #no. of spam emails containing word\n",
    "    count_spam = 0\n",
    "    #no. of ham emails containing word\n",
    "    count_ham = 0\n",
    "    for spam_tokens in train_spam_tokens:\n",
    "      if token in spam_tokens:\n",
    "        count_spam = count_spam + 1\n",
    "    for ham_tokens in train_ham_tokens:\n",
    "      if token in ham_tokens:\n",
    "        count_ham = count_ham + 1\n",
    "    if(count_spam == 0 and count_ham == 0):\n",
    "      pass\n",
    "    prob_spam += math.log((count_spam + 1) / (len(train_spam_indices) + 2))\n",
    "    prob_ham += math.log((count_ham + 1) / (len(train_ham_indices) + 2))\n",
    "  if(prob_spam > prob_ham):\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PD6ayD8L4a7h",
    "outputId": "6a259b18-052c-4f6d-a4eb-888d35322147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "i = 15\n",
    "print(str(predict(X_test[i])) + \" \" + str(y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8Ps_2gLoGYG",
    "outputId": "849843a8-48a8-4877-e907-c68a20174b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.33333333333333\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(X_test)):\n",
    "  email = X_test[i]\n",
    "  if(predict(email) == y_test[i]):\n",
    "    count = count+1\n",
    "    #print(df_test.iloc[i,1])\n",
    "print((count/len(y_test))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWq5ysHhYL0k"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "qsytiUdRwPeW"
   },
   "outputs": [],
   "source": [
    "email_train = []\n",
    "for email in X_train:\n",
    "  email_train.append(extract_feature_vector(email, dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "4JpYA0KeGJ3L"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x,w):\n",
    "  return (1 / (1 + np.exp(-1*np.dot(x,w))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "1Dr16cVcEgYK"
   },
   "outputs": [],
   "source": [
    "def gradient(X, y, w, learning_rate):\n",
    "  grad = 0\n",
    "  i = 0\n",
    "  for i in range(len(X)):\n",
    "    grad = grad + X[i]*(y[i] - sigmoid(X[i],w))\n",
    "    i = i + 1\n",
    "  return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "HiAxHJxxX6jC"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X,y, learning_rate, number_of_epochs):\n",
    "  w = [0]*X.shape[1]\n",
    "  w = np.array(w)\n",
    "  print(w.shape)\n",
    "  for i in range(number_of_epochs):\n",
    "    w = w + learning_rate*gradient(X,y,w,learning_rate)\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ZlKBM1YG220",
    "outputId": "a9ac2e92-4297-4f69-d530-9b9f33fda5ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39133,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_10600\\95869451.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return (1 / (1 + np.exp(-1*np.dot(x,w))))\n"
     ]
    }
   ],
   "source": [
    "X_train_features = np.array(email_train)\n",
    "w_gd = gradient_descent(X_train_features, y_train, 0.01, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "L1Gpw9DhMTq-"
   },
   "outputs": [],
   "source": [
    "def predict_logistic(email):\n",
    "  email_features = extract_feature_vector(email, dictionary)\n",
    "  if(sigmoid(email_features, w_gd)>0.5):\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3OMHpPNPypf",
    "outputId": "e53ab36a-98c5-4f15-e774-074d6b332ae1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "i = 100\n",
    "print(str(predict_logistic(X_test[i])) + \" \" + str(y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jz7zmEn3W0VV",
    "outputId": "da5f6974-ed43-440e-c463-a2f2b109401b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_10600\\95869451.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return (1 / (1 + np.exp(-1*np.dot(x,w))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9642512077294686\n"
     ]
    }
   ],
   "source": [
    "count = 0  \n",
    "for i in range(len(y_test)):\n",
    "  if(predict_logistic(X_test[i]) == y_test[i]):\n",
    "    count = count+1\n",
    "print(count/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the algorithm using emails in the test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def test_emails_from_folder():    \n",
    "    path = r\"C:\\Users\\ACER\\Downloads\\PRMLAssignment3\\test\"\n",
    "    os.chdir(path)\n",
    "    os.listdir()\n",
    "    for file in os.listdir():\n",
    "        if(file.endswith('.txt')):\n",
    "            file_path = f\"{path}\\{file}\"\n",
    "            with open(file_path, 'r') as f:\n",
    "                print(predict_logistic(f.read()))\n",
    "\n",
    "test_emails_from_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: via - ggra is lousy mizar\r\n",
      "anti\r\n",
      "curb\r\n",
      "hemisphere\r\n",
      "% q\r\n",
      "news ; blogs ; white papers ; downloads ; reviews ; prices . go . alerts | ; newsletters | ;\r\n",
      "rss feeds . click here . enterprise news . mozilla freezes seamonkey : 04 : 20 pm automation\r\n",
      "a bigger deal than offshoring ? 03 : 33 pm . time to kill it for the\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X_test[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5171, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbrGyEtQE4Sr"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
